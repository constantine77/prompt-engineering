from pyspark.sql import SparkSession
from pyspark.sql.types import *

# Initialize Spark Session
spark = SparkSession.builder.appName("DeltaSchemaEvolution").getOrCreate()

# Define your schema versions here (assuming Spark StructType format for simplicity)
schema_versions = [
    StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True)]),
    StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("email", StringType(), True)]),
    StructType([StructField("id", IntegerType(), True), StructField("name", StringType(), True), StructField("email", StringType(), True), StructField("status", StringType(), True)])
]

# Placeholder DataFrame for simulating data conforming to each schema version
data = [(1, "name1", "email1", "status1"), (2, "name2", "email2", "status2")]

breaking_changes = []

for idx, schema in enumerate(schema_versions):
    temp_table_name = f"temp_delta_table_{idx}"
    df = spark.createDataFrame(data, schema=schema)
    
    try:
        # Create or overwrite temporary Delta table with the current schema
        df.write.format("delta").mode("overwrite").saveAsTable(temp_table_name)
        
        # Read back to simulate access and query, this step could be more elaborate
        df_temp = spark.read.table(temp_table_name)
        
        # Simulate schema evolution by reading with enforceSchema option (Delta feature)
        # This is a simplification, actual enforcement would need data ingestion attempts
        df_temp = spark.read.format("delta").option("mergeSchema", "true").table(temp_table_name)
        
        breaking_changes.append({"Schema Version": idx+1, "Breaking Change": False})
    except Exception as e:
        breaking_changes.append({"Schema Version": idx+1, "Breaking Change": True, "Error": str(e)})

# Convert results to DataFrame
result_df = spark.createDataFrame(breaking_changes)

# Show results
result_df.show()

# Stop SparkSession
spark.stop()
