from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType
import json

# Initialize SparkSession with Delta Lake support
spark = SparkSession.builder \
    .appName("Delta Schema Breaking Changes Detector") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:1.0.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Define sample Avro schemas as JSON strings
avro_schema_str_1 = '{"type":"record","name":"TestSchema1","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"}]}'
avro_schema_str_2 = '{"type":"record","name":"TestSchema2","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"},{"name":"age","type":"int"}]}'
avro_schema_str_3 = '{"type":"record","name":"TestSchema3","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"},{"name":"age","type":["int","null"]}]}'

# Function to convert Avro schema string to Spark DataFrame schema
def avro_to_spark_schema(avro_schema_str):
    avro_schema = json.loads(avro_schema_str)
    fields = avro_schema['fields']
    spark_fields = []
    for field in fields:
        field_name = field['name']
        field_type = field['type']
        if field_type == "string":
            spark_fields.append(StructField(field_name, StringType(), True))
        elif field_type == "int":
            spark_fields.append(StructField(field_name, IntegerType(), True))
        # Add more type conversions as needed
    return StructType(spark_fields)

# Convert Avro schemas to Spark DataFrame schemas
schema_versions = [
    avro_to_spark_schema(avro_schema_str_1),
    avro_to_spark_schema(avro_schema_str_2),
    avro_to_spark_schema(avro_schema_str_3)
]

# Sample data to apply schemas to
data = [(1, "Alice", 30), (2, "Bob", 25), (3, "Charlie", 35)]
df = spark.createDataFrame(data, ["id", "name", "age"])

# Apply each schema to the Delta table and identify breaking changes
breaking_changes = []

for idx, schema in enumerate(schema_versions):
    # Temporarily change logic to simulate applying schema changes
    try:
        # Simulate applying schema changes by selecting columns based on the schema
        temp_df = df.select([field.name for field in schema.fields])
        breaking_changes.append({"Schema Version": idx + 1, "Breaking Change": False})
    except Exception as e:
        breaking_changes.append({"Schema Version": idx + 1, "Breaking Change": True, "Error": str(e)})

# Convert results to DataFrame
result_df = spark.createDataFrame(breaking_changes)

# Show results
result_df.show()

# Stop SparkSession
spark.stop()
