Automating recommendations on executed Spark jobs involves analyzing metrics and logs to identify patterns, inefficiencies, and areas of improvement. Here's a step-by-step guide on how to approach this:

1. **Data Collection**:
   - Gather all relevant metrics using tools like `sparkMeasure`, Spark listeners, and other monitoring solutions.
   - Capture logs, especially those with WARN or ERROR levels.

2. **Data Aggregation**:
   - Use a centralized system (like ELK stack - Elasticsearch, Logstash, Kibana) to aggregate and visualize your metrics and logs.
   - Structured logging can be beneficial as it allows for easier querying and analysis.

3. **Define Performance Benchmarks**:
   - Determine what "good" performance looks like for your jobs. This can be based on historical data, industry standards, or specific organizational targets.

4. **Analyze Data**:
   - Identify outliers, patterns, and deviations from benchmarks.
   - Use statistical methods or machine learning models to detect anomalies or trends.

5. **Generate Recommendations**:

   - **Resource Allocation**: If tasks are frequently pending due to a lack of resources, recommend increasing executor memory, core count, or the number of executors.
   - **Data Skew**: If there's a significant difference in task completion times within the same stage, it might indicate data skew. Recommend repartitioning or using techniques like Salting.
   - **Storage**: If reading from or writing to storage is a bottleneck, recommend using a faster storage format (like Parquet), adjusting Spark's `blockSize`, or optimizing storage layer configurations.
   - **Shuffling**: High shuffle write or read can be a performance killer. Recommend reducing shuffle operations, considering broadcast joins, or increasing the shuffle partition count.
   - **Errors & Failures**: If specific errors or failures occur frequently, provide specific solutions or best practices to address them.
   - **Optimize Configurations**: Adjust configurations like `spark.driver.memory`, `spark.executor.memory`, `spark.shuffle.service.enabled`, and others based on observed job performance.

6. **Automation**:
   - Use scheduled scripts or tools to run your analysis periodically.
   - Consider developing a custom application or script that ingests metrics, applies analysis algorithms, and outputs recommendations.
   - Integrate with alerting systems to notify users or administrators when specific conditions are met or when recommendations are generated.

7. **Machine Learning for Advanced Recommendations**:
   - Train ML models on your metrics to predict job failures, slow performance, or other issues.
   - Use regression, clustering, or classification algorithms based on the type of recommendations and predictions you want to make.
   - Over time, as more data gets collected, retrain your models to improve their accuracy.

8. **Feedback Loop**:
   - Allow users or administrators to provide feedback on the generated recommendations. This can help refine and prioritize which suggestions are most beneficial.
   - Use this feedback to improve the recommendation algorithms and adjust thresholds or rules as needed.

9. **Documentation & Training**:
   - Alongside automated recommendations, maintain a repository of documentation and best practices. This can guide users in implementing the recommendations and understanding their significance.
   - Conduct training sessions or workshops to educate developers and administrators on Spark optimizations.

Remember, the effectiveness of your automated recommendation system will improve over time as more data gets collected and as feedback is integrated. Periodic reviews and adjustments are essential to keep the system relevant and beneficial.