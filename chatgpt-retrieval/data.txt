from pyspark.sql import SparkSession
from pyspark.sql.types import StructType
import json

# Initialize Spark Session (Assuming Databricks environment)
spark = SparkSession.builder.appName("DeltaSchemaMerge").getOrCreate()

# Function to convert JSON string to StructType
def json_to_struct(json_str):
    return StructType.fromJson(json.loads(json_str))

# Mock Avro schemas as JSON strings for each scenario
original_schema_str = '{"type":"record","name":"BaseRecord","fields":[{"name":"id","type":"int"},{"name":"field1","type":"string","default":"default_value"}]}'
name_change_schema_str = '{"type":"record","name":"NameChangeRecord","fields":[{"name":"id","type":"int"},{"name":"field2","type":"string","default":"default_value"}]}'
type_change_schema_str = '{"type":"record","name":"TypeChangeRecord","fields":[{"name":"id","type":"string"},{"name":"field1","type":"string","default":"default_value"}]}'
default_value_change_schema_str = '{"type":"record","name":"DefaultValueChangeRecord","fields":[{"name":"id","type":"int"},{"name":"field1","type":"string","default":"new_default_value"}]}'

# Convert JSON schemas to Spark DataFrame schemas
original_schema = json_to_struct(original_schema_str)
name_change_schema = json_to_struct(name_change_schema_str)
type_change_schema = json_to_struct(type_change_schema_str)
default_value_change_schema = json_to_struct(default_value_change_schema_str)

# Base DataFrame matching the original schema
df_base = spark.createDataFrame([], schema=original_schema)

# Write the base DataFrame to Delta
delta_path = "/mnt/delta/base_table"
df_base.write.format("delta").mode("overwrite").save(delta_path)

# Function to try merging schema with the Delta table
def try_merge_schema(delta_path, modified_schema, scenario):
    df_modified = spark.createDataFrame([], schema=modified_schema)
    
    try:
        # Attempt to merge schema with existing Delta table
        df_modified.write.format("delta").mode("append").option("mergeSchema", "true").save(delta_path)
        print(f"{scenario}: Schema merge successful, no breaking change detected.")
    except Exception as e:
        print(f"{scenario}: Schema merge failed, breaking change detected. Error: {str(e)}")

# Attempt to merge schemas for each scenario
try_merge_schema(delta_path, name_change_schema, "Field Name Change")
try_merge_schema(delta_path, type_change_schema, "Field Data Type Change")
try_merge_schema(delta_path, default_value_change_schema, "Field Default Value Change")

# Stop Spark Session at the end of execution (Optional in Databricks)
spark.stop()


####################
To create a solution for identifying Delta schema breaking changes in Databricks using Python, follow these steps. This approach leverages the `mergeSchema` option in Delta Lake to test schema compatibility.

### Step 1: Set Up Mock Avro Schemas

First, prepare your Avro schema definitions for each scenario in JSON format.

```json
// Original Schema
{
  "type": "record",
  "name": "BaseRecord",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "field1", "type": "string", "default": "default_value"}
  ]
}

// A. Field Name Change
{
  "type": "record",
  "name": "NameChangeRecord",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "field2", "type": "string", "default": "default_value"}
  ]
}

// B. Field Data Type Change
{
  "type": "record",
  "name": "TypeChangeRecord",
  "fields": [
    {"name": "id", "type": "string"},
    {"name": "field1", "type": "string", "default": "default_value"}
  ]
}

// C. Field Default Value Change
{
  "type": "record",
  "name": "DefaultValueChangeRecord",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "field1", "type": "string", "default": "new_default_value"}
  ]
}
```

### Step 2: Convert JSON Schemas to Spark DataFrame Schemas

Use PySpark to read the JSON schemas and convert them into Spark DataFrame schemas. This example assumes you have the JSON schema stored as strings or loaded from files.

```python
from pyspark.sql.types import StructType
import json

# Mock function to convert JSON string to StructType
def json_to_struct(json_str):
    return StructType.fromJson(json.loads(json_str))

# Example conversion
original_schema_str = '{"type":"record","name":"BaseRecord","fields":[{"name":"id","type":"int"},{"name":"field1","type":"string","default":"default_value"}]}'
original_schema = json_to_struct(original_schema_str)
```

### Step 3: Create DataFrames and Apply Schemas

Create data for each scenario, matching the original and modified schemas.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Initialize Spark Session (Assuming Databricks environment)
spark = SparkSession.builder.appName("DeltaSchemaMerge").getOrCreate()

# Base DataFrame matching the original schema
df_base = spark.createDataFrame([], schema=original_schema)

# Write the base DataFrame to Delta
delta_path = "/mnt/delta/base_table"
df_base.write.format("delta").mode("overwrite").save(delta_path)
```

### Step 4: Apply Modified Schemas Using mergeSchema

Try appending data with a modified schema using the `mergeSchema` option and catch any exceptions to identify breaking changes.

```python
def try_merge_schema(delta_path, modified_schema_str, scenario):
    modified_schema = json_to_struct(modified_schema_str)
    df_modified = spark.createDataFrame([], schema=modified_schema)
    
    try:
        # Attempt to merge schema with existing Delta table
        df_modified.write.format("delta").mode("append").option("mergeSchema", "true").save(delta_path)
        print(f"{scenario}: Schema merge successful, no breaking change detected.")
    except Exception as e:
        print(f"{scenario}: Schema merge failed, breaking change detected. Error: {str(e)}")

# Define your modified schema strings for scenarios A, B, and C
name_change_schema_str = '...'
type_change_schema_str = '...'
default_value_change_schema_str = '...'

# Attempt to merge schemas for each scenario
try_merge_schema(delta_path, name_change_schema_str, "Field Name Change")
try_merge_schema(delta_path, type_change_schema_str, "Field Data Type Change")
try_merge_schema(delta_path, default_value_change_schema_str, "Field Default Value Change")
```

### Summary

This solution involves preparing mock Avro schemas for different change scenarios, converting them into Spark DataFrame schemas, and attempting to append these schemas to an existing Delta table with the `mergeSchema` option enabled. Catching exceptions during the merge attempts helps identify incompatible schema changes, effectively detecting breaking changes within a Databricks environment using Python.