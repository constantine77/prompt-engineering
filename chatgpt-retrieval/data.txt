import json
from pyspark.sql.session import SparkSession
from pyspark.sql.types import *
from pyspark.sql.functions import lit

# Initialize Spark Session
spark = SparkSession.builder.appName("DeltaSchemaEvolution").getOrCreate()

# Sample Avro schema strings
avro_schema_v1_str = '{"type":"record","name":"MyRecord","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"}]}'
avro_schema_v2_str = '{"type":"record","name":"MyRecord","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"},{"name":"email","type":["null","string"]}]}'
avro_schema_v3_str = '{"type":"record","name":"MyRecord","fields":[{"name":"id","type":"int"},{"name":"name","type":"string"},{"name":"email","type":["null","string"]},{"name":"status","type":"string"}]}'

# Function to load JSON schema and convert it to Spark DataFrame schema
def load_json_schema(avro_schema_str):
    return json.loads(avro_schema_str)

# Mapping function from Avro to Spark data types
def avro_type_to_spark_type(avro_type):
    type_mappings = {
        "int": IntegerType(),
        "string": StringType(),
        "long": LongType(),
        "double": DoubleType(),
        "float": FloatType(),
        "boolean": BooleanType(),
        "bytes": BinaryType(),
        # Add more mappings as needed
    }
    return type_mappings.get(avro_type, StringType())  # Default to StringType if unknown

# Function to convert JSON schema (representing Avro schema) to Spark DataFrame schema
def json_to_spark_schema(json_schema):
    fields = []
    for field in json_schema.get('fields', []):
        if isinstance(field['type'], list):  # Nullable types
            spark_type = avro_type_to_spark_type(field['type'][1])  # Assuming second type is the non-null type
        else:
            spark_type = avro_type_to_spark_type(field['type'])
        fields.append(StructField(field['name'], spark_type, True))
    return StructType(fields)

# Assuming avro_schema_v1_str, avro_schema_v2_str, and avro_schema_v3_str are defined as above
avro_schemas_str = [avro_schema_v1_str, avro_schema_v2_str, avro_schema_v3_str]

# Convert Avro schemas to Spark schemas
spark_schemas = [json_to_spark_schema(load_json_schema(schema_str)) for schema_str in avro_schemas_str]

# Create a temporary Delta table and apply schema changes to identify breaking changes
temp_table_name = "temp_delta_table"
for idx, spark_schema in enumerate(spark_schemas):
    df = spark.createDataFrame([], schema=spark_schema)
    df = df.withColumn("schema_version", lit(idx+1))  # Adding schema version for tracking
    if idx == 0:
        df.write.format("delta").mode("overwrite").saveAsTable(temp_table_name)
    else:
        df.write.format("delta").mode("append").saveAsTable(temp_table_name)

    print(f"Applied schema version {idx+1} to the temporary Delta table.")

# Assuming this is a simplified example. In practice, you would use the Delta table's history
# to identify if any schema evolution caused incompatibilities or breaking changes.
