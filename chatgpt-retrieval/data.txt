from pyspark.sql import SparkSession

# Create a Spark session with Delta Lake support
spark = SparkSession.builder \
    .appName("DeltaLakeSchemaReader") \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:1.0.0") \
    .getOrCreate()

# Mock Delta Lake data for the sake of this example
data = [
    ("John", 28, "New York"),
    ("Mike", 22, "Los Angeles"),
    ("Sara", 30, "Chicago")
]
df = spark.createDataFrame(data, ["name", "age", "city"])
delta_path = "/tmp/delta-table"
df.write.format("delta").save(delta_path)

# Read the Delta Lake table schema
delta_df = spark.read.format("delta").load(delta_path)
print(delta_df.schema)

spark.stop()
