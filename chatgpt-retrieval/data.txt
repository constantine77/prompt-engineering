from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Detecting Delta Lake Breaking Changes") \
    .getOrCreate()

# Sample schema versions with breaking changes
schema_versions = [
    StructType([StructField("name", StringType(), True), StructField("age", IntegerType(), True)]),  # Initial schema
    StructType([StructField("name", StringType(), True), StructField("age", IntegerType(), True), StructField("city", StringType(), True)]),  # Breaking change: Added 'city'
    StructType([StructField("name", StringType(), True), StructField("age", IntegerType(), True), StructField("city", StringType(), True), StructField("country", StringType(), True)])  # Breaking change: Added 'country'
]

# Create DataFrame with sample data
data = [("Alice", 30), ("Bob", 25)]
df = spark.createDataFrame(data, ["name", "age"])

# Apply each schema to the Delta table and identify breaking changes
breaking_changes = []

for idx, schema in enumerate(schema_versions):
    try:
        # Create temporary Delta table with the current schema
        df.write.format("delta").mode("overwrite").saveAsTable("temp_delta_table")
        
        # Attempt to apply schema changes
        df_temp = spark.read.format("delta").table("temp_delta_table")
        df_temp = df_temp.selectExpr(*[f"`{field.name}` as `{field.name}`" for field in schema.fieldNames()])
        
        # If no errors occur, schema change is compatible
        breaking_changes.append({"Schema Version": idx, "Breaking Change": False})
    except Exception as e:
        # If errors occur, schema change is breaking
        breaking_changes.append({"Schema Version": idx, "Breaking Change": True, "Error": str(e)})

# Convert results to DataFrame
result_df = spark.createDataFrame(breaking_changes)

# Show results
result_df.show()

# Stop SparkSession
spark.stop()
